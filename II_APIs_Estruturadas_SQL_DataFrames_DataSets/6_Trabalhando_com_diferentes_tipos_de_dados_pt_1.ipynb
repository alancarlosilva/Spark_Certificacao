{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 6 - Diferentes tipos de dados - Parte 1\n",
    "\n",
    "Esta parte foca em construir expressões, conhecer e revisar os tipos de dados ja conhecidos e novos do Spark. Abaixo seguem algumas dicas de materiais complementares para encontrar funções para transformação de dados:\n",
    "\n",
    "https://spark.apache.org/docs/2.3.0/api/java/index.html?org/apache/spark/sql/Dataset.html\n",
    "\n",
    "https://spark.apache.org/docs/2.4.0/sql-programming-guide.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/sql/index.html\n",
    "\n",
    "### Sumário\n",
    " 1. __Boleanos__\n",
    " 2. __Números__\n",
    " 3. __Strings__\n",
    " 4. __Datas e timestamps__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obs.__: Por convenção, DF..., df..., df_... , são variáveis que armazenam DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conjunto de dados usados para os exemplos que seguem\n",
    "path = \"file:///root/Spark_Certificacao/data/retail-data/by-day/2010-12-01.csv\"\n",
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(path)\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertendo dados para os tipos do Spark\n",
    "\n",
    "Uma tarefa que realizaremos é converter tipos nativos em tipos Spark. Fazemos isso usando a primeira função que apresentamos aqui, a função __lit__. Esta função converte um tipo de dado de outra linguagem para sua equivalente no Spark. Veja como podemos converter alguns tipos diferentes de valores do Python para seus respectivos tipos de Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'float'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Tipos de dados do Python\n",
    "print type(5)\n",
    "print type(\"five\")\n",
    "print type(5.0)\n",
    "\n",
    "# Usando a função lit para converter em Spark\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boleanos\n",
    "\n",
    "Booleanos são essenciais quando se trata de análise de dados, porque eles são a base para toda a filtragem. As declarações booleanas consistem em quatro elementos: ___and___, ___or___, ___true___ e ___false___. Usamos essas estruturas simples para criar instruções lógicas que são avaliadas como verdadeiras ou falsas. Essas instruções são frequentemente usadas como requisitos condicionais para quando uma linha de dados deve passar no teste (avaliar para verdadeiro) ou então será filtrada.\n",
    "\n",
    "Operadores lógicos em Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "&  # and\n",
    "|  # or\n",
    "~  # not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.where(col(\"InvoiceNo\") != 536365)\\\n",
    ".select(\"InvoiceNo\", \"Description\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra opção é especificar o predicado como uma expressão em uma string. Isso é válido para Python ou Scala. Note que isso também dá acesso a outra maneira de expressar \"não é igual\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536366   |22633    |HAND WARMER UNION JACK       |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536366   |22632    |HAND WARMER RED POLKA DOT    |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT|32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|\n",
      "|536367   |22745    |POPPY'S PLAYHOUSE BEDROOM    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "|536367   |22748    |POPPY'S PLAYHOUSE KITCHEN    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"InvoiceNo <> 536365\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode especificar expressões booleanas com várias partes quando você usa ___and___ ou ___not___. No Spark, você deve sempre encadear o ___and___ e filtrar como um filtro sequencial. A razão para isso é que mesmo que as declarações booleanas sejam expressas em série (uma após a outra), o Spark achatará todos esses filtros em uma instrução e executará o filtro ao mesmo tempo, criando a declaração ___and___ para nós. Embora você possa especificar suas declarações explicitamente usando ___and___, se desejar, elas serão mais fáceis de entender e ler se forem especificadas em série. A instrução ___or___ precisa ser especificada na mesma declaração:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT * FROM dfTable WHERE StockCode in (\"DOT\") AND(UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*A função __instr__ retorna o índice da primeira ocorrência de uma substring dada uma string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expressões booleanas não são reservadas apenas para filtros. Para filtrar um DataFrame, você também pode especificar apenas uma coluna booleana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|unitPrice|isExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "\n",
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n",
    "                .where(\"isExpensive\")\\\n",
    "                .select(\"unitPrice\", \"isExpensive\").show(5)\n",
    "\n",
    "# SELECT UnitPrice, (StockCode = 'DOT' AND\n",
    "#        (UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1)) as isExpensive\n",
    "#        FROM dfTable\n",
    "#        WHERE (StockCode = 'DOT' AND\n",
    "#        (UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe como não precisávamos especificar nosso filtro como uma expressão e como poderíamos usar um nome de coluna sem nenhum trabalho extra. Se você estiver acostumado com SQL, todas essas instruções devem parecer bastante familiares. De fato, todos eles podem ser expressos como uma cláusula __where__. Na verdade, muitas vezes é mais fácil apenas expressar os filtros como instruções SQL do que usar a interface programática do DataFrame e o Spark SQL nos permite fazer isso sem pagar qualquer penalidade de desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|   Description|UnitPrice|\n",
      "+--------------+---------+\n",
      "|DOTCOM POSTAGE|   569.77|\n",
      "|DOTCOM POSTAGE|   607.49|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\\\n",
    "                .where(\"isExpensive\")\\\n",
    "                .select(\"Description\", \"UnitPrice\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma pegadinha que pode surgir é quando você estiver trabalhando com dados nulos ao criar expressões booleanas. Se houver um nulo em seus dados, você precisará tratar as coisas de maneira um pouco diferente. Veja como garantir a realização de um teste de equivalência com segurança nula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"Description\").eqNullSafe(\"hello\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Números\n",
    "\n",
    "A segunda tarefa mais comum que você fará depois de filtrar as coisas é contar as coisas. Na maioria das vezes, precisamos apenas expressar nossa computação, e isso deve ser válido supondo que estamos trabalhando com tipos de dados numéricos. Vamos imaginar que, descobrimos que não registramos a quantidade de dados de varejo em nosso conjunto de dados e, a quantidade real é igual a:<br>\n",
    "    <b>(a quantidade atual * o preço unitário)^2 + 5.</b> \n",
    "<br>\n",
    "Isso também introduzirá nossa primeira função numérica como a função potência que eleva uma coluna a potência desejada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, pow\n",
    "\n",
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repare que fomos capazes de multiplicar nossas colunas porque ambas eram numéricas. Naturalmente, podemos adicionar e subtrair, se necessário, também. Na verdade, podemos fazer tudo isso como um SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT customerId, (POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\n",
    "# FROM dfTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Arredondando Valores\n",
    "from pyspark.sql.functions import lit, round, bround\n",
    "\n",
    "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT round(2.5), bround(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funções estatísticas podem ser aplicadas também. \n",
    "# Correlação entre duas colunas:\n",
    "from pyspark.sql.functions import corr\n",
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT corr(Quantity, UnitPrice) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra tarefa comum é calcular estatísticas-resumo de uma coluna ou conjunto de colunas. Podemos usar o método ___describe___ para fazer isso. Isso calculará as colunas __numéricas__ trazendo: média, desvio padrão, valor min e valor max. Você deve usar isso principalmente para visualização no console porque o esquema pode ser mutável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|          Quantity|         UnitPrice|        CustomerID|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|              3108|              3108|              1968|\n",
      "|   mean| 8.627413127413128| 4.151946589446603|15661.388719512195|\n",
      "| stddev|26.371821677029203|15.638659854603892|1854.4496996893627|\n",
      "|    min|               -24|               0.0|           12431.0|\n",
      "|    max|               600|            607.49|           18229.0|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(\"Quantity\",\"UnitPrice\",\"CustomerID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há um número de funções estatísticas disponíveis no pacote __StatFunctions__ (acessível usando __stat__ como vemos no bloco de código abaixo). Estes são os métodos DataFrame que você pode usar para calcular uma variedade de coisas diferentes. Por exemplo, você pode calcular quantis exatos ou aproximados de seus dados usando o método ___approxQuantile___:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colName = \"UnitPrice\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Você também pode usar isso para ver uma tabulação cruzada (reduzidos pelo tamanho da saída):\n",
    "df.stat.crosstab(\"StockCode\", \"Quantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "| StockCode_freqItems|  Quantity_freqItems|\n",
      "+--------------------+--------------------+\n",
      "|[90214E, 20728, 2...|[200, 128, 23, 32...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Itens frequentes:\n",
    "df.stat.freqItems([\"StockCode\", \"Quantity\"]).show(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função ___monotonically_increasing_id___ gera um valor único para cada linha, começando com 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "+-----------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df.select(monotonically_increasing_id()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Identificador|\n",
      "+-------------+\n",
      "|            0|\n",
      "|            1|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df.select(monotonically_increasing_id().alias(\"Identificador\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings\n",
    "\n",
    "A manipulação de strings é mostrada em quase todos os fluxos de dados, e vale a pena explicar o que você pode fazer com as strings. Você pode estar manipulando arquivos de log executando extração ou substituição de expressões regulares ou verificando a existência de strings simples ou tornando todas as strings maiúsculas ou minúsculas. Vamos começar com a última tarefa porque é a mais simples. A função ___initcap___ irá capitalizar a primeira letra de cada palavra em uma determinada string quando essa palavra é separada de outra por um espaço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|initcap(Description)               |\n",
      "+-----------------------------------+\n",
      "|White Hanging Heart T-light Holder |\n",
      "|White Metal Lantern                |\n",
      "|Cream Cupid Hearts Coat Hanger     |\n",
      "|Knitted Union Flag Hot Water Bottle|\n",
      "|Red Woolly Hottie White Heart.     |\n",
      "+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(col(\"Description\"))).show(5, False)\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT initcap(Description) FROM dfTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "|Description                       |lower(Description)                |upper(lower(Description))         |\n",
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|white hanging heart t-light holder|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |white metal lantern               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conversão para maiúsculas e minúsculas\n",
    "from pyspark.sql.functions import lower, upper\n",
    "df.select(col(\"Description\"), lower(col(\"Description\")), upper(lower(col(\"Description\")))).show(2, False)\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT Description, lower(Description), Upper(lower(Description)) FROM dfTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---+----------+\n",
      "| ltrim| rtrim| trim| lp|        rp|\n",
      "+------+------+-----+---+----------+\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "+------+------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(\n",
    "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),              # Removendo espaços a esquerda\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),              # Removendo espaços a direita   \n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),                # Removendo espaços dos dois lados \n",
    "lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),            \n",
    "rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)\n",
    "\n",
    "# Dado o numero inteiro (n) passado como parâmetro e uma string (s), lpad faz:\n",
    "# n - len(s) = string mais n espaços.\n",
    "# Caso o numero (n) seja menor que a string, ele remove caracteres do lado direito da mesma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressões regulares\n",
    "\n",
    "Provavelmente, uma das tarefas executadas com mais frequência é pesquisar a existência de uma string em outra ou substituir todas as menções de uma string por outro valor. Isso geralmente é feito com uma ferramenta chamada ___expressões regulares___ que existe em muitas linguagens de programação. Expressões regulares dão ao usuário a capacidade de especificar um conjunto de regras a serem usadas para extrair valores de uma string ou substituí-los por outros valores.\n",
    "\n",
    "O Spark aproveita todo o poder das expressões regulares do Java. A sintaxe de expressões regulares do Java se distancia um pouco das outras linguagens de programação, portanto vale a pena revisar antes de colocar qualquer coisa em produção. Existem duas funções principais no Spark que você precisa para executar tarefas de expressão regular: <b>regexp_extract</b> e <b>regexp_replace</b>. Essas funções extraem valores e substituem valores, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|color_clean                       |Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|COLOR HANGING HEART T-LIGHT HOLDER|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|COLOR METAL LANTERN               |WHITE METAL LANTERN               |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |CREAM CUPID HEARTS COAT HANGER    |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex_string = \"RED|WHITE|BLACK|GREEN|BLUE\"\n",
    "df.select(\n",
    "regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n",
    "col(\"Description\")).show(3, False)\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT\n",
    "#     regexp_replace(Description, 'BLACK|WHITE|RED|GREEN|BLUE', 'COLOR') as\n",
    "#     color_clean, Description\n",
    "# FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra tarefa pode ser substituir determinados caracteres por outros caracteres. Construir isso como uma expressão regular pode ser entediante, portanto, o Spark também fornece a função ___translate___ para substituir esses valores. Isso é feito no nível do caractere e substituirá todas as instâncias de um caractere pelo caractere indexado na cadeia de substituição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|translate(Description, LEET, 1337)|Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHI73 HANGING H3AR7 7-1IGH7 HO1D3R|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHI73 M37A1 1AN73RN               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1337\"),col(\"Description\")).show(2, False)\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT translate(Description, 'LEET', '1337'), Description FROM dfTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|color_clean|         Description|\n",
      "+-----------+--------------------+\n",
      "|      WHITE|WHITE HANGING HEA...|\n",
      "|      WHITE| WHITE METAL LANTERN|\n",
      "+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Também podemos realizar algo semelhante, como exibir a primeira cor mencionada\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(\n",
    "regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\n",
    "col(\"Description\")).show(2)\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT regexp_extract(Description, '(BLACK|WHITE|RED|GREEN|BLUE)', 1),\n",
    "#     Description\n",
    "# FROM dfTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Às vezes, em vez de extrair valores, simplesmente queremos verificar sua existência. Podemos fazer isso com o método \n",
    "# contains em cada coluna. Isso retornará um booleano declarando se o valor especificado está na string da coluna:\n",
    "from pyspark.sql.functions import instr\n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
    "    .where(\"hasSimpleColor\")\\\n",
    "    .select(\"Description\").show(3, False)\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT Description FROM dfTable\n",
    "# WHERE instr(Description, 'BLACK') >= 1 OR instr(Description, 'WHITE') >= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos trabalhar com isso de maneira mais rigorosa e aproveitar a capacidade do Spark de aceitar um número dinâmico de argumentos. Quando convertemos uma lista de valores em um conjunto de argumentos e os passamos para uma função, usamos um recurso de linguagem chamado ___varargs___. Usando esse recurso, podemos desvendar efetivamente um array de tamanho arbitrário e passá-lo como argumentos para uma função. Isso, juntamente com o select, permite que criemos números arbitrários de colunas dinamicamente.\n",
    "\n",
    "Com o Python, vamos usar uma função diferente, locate, que retorna o local inteiro. Em seguida, convertemos isso em um booleano antes de usá-lo como o mesmo recurso básico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, locate\n",
    "\n",
    "simpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "\n",
    "def color_locator(column, color_string):\n",
    "    return locate(color_string.upper(), column).cast(\"boolean\").alias(\"is_\" + c)\n",
    "\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "selectedColumns.append(expr(\"*\"))\n",
    "\n",
    "df.select(*selectedColumns).where(expr(\"is_white OR is_red\")).select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datas e Timestamps\n",
    "\n",
    "Datas e horários são um desafio constante em linguagens de programação e bancos de dados. É sempre necessário acompanhar os fusos horários e garantir que os formatos estejam corretos e válidos. O Spark faz o melhor para manter as coisas simples, concentrando-se explicitamente em dois tipos de informações relacionadas ao tempo. Há datas, que se concentram exclusivamente em datas de calendário e __timestamps__, que incluem informações de data e hora. O Spark, como vimos com nosso conjunto de dados atual, fará o melhor esforço para identificar corretamente os tipos de colunas, incluindo datas e carimbos de data e hora quando ativarmos o inferSchema.\n",
    "\n",
    "Podemos ver que isso funcionou muito bem com nosso conjunto de dados atual, pois foi capaz de identificar e ler nosso formato de data sem que precisássemos fornecer alguma especificação para isso. Como sugerimos anteriormente, trabalhar com datas e __timestamps__ se relaciona de perto ao trabalho com strings porque geralmente armazenamos nossos timestamps ou datas como strings e as convertemos em tipos de data em tempo de execução. Isso é menos comum quando se trabalha com bancos de dados e dados estruturados, mas é muito mais comum quando estamos trabalhando com arquivos de texto e CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora o Spark faça a leitura de datas ou horas com base no melhor esforço, às vezes não haverá como trabalhar com datas e horários estranhamente formatados. A chave para entender as transformações que você precisará aplicar é garantir que você saiba exatamente que tipo e formato você tem em cada etapa do caminho. Outra pegadinha comum é que a classe ___TimestampType___ do Spark oferece suporte apenas a precisão de segundo nível, o que significa que, se você estiver trabalhando com milissegundos ou microssegundos, precisará solucionar esse problema operando com base neles a longo prazo. \n",
    "\n",
    "Qualquer precisão maior ao coagir para um ___TimestampType___ será removida. O Spark pode ser um pouco específico sobre o formato que você tem em um determinado ponto no tempo. É importante ser explícito ao analisar ou converter para garantir que não haja problemas ao fazê-lo. No final do dia, o Spark está trabalhando com datas e timestamps Java e, portanto, está em conformidade com esses padrões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = spark.range(10)\\\n",
    "    .withColumn(\"today\", current_date())\\\n",
    "    .withColumn(\"now\", current_timestamp())\n",
    "\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.printSchema()\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2019-05-07|        2019-05-17|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agora que temos um DataFrame simples para trabalhar, vamos adicionar e subtrair cinco dias a partir de hoje. \n",
    "# Essas funções selecionam uma coluna e, em seguida, o número de dias para adicionar ou subtrair como os argumentos:\n",
    "\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT date_sub(today, 5), date_add(today, 5) FROM dateTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------+\n",
      "|months_between(start, end)|\n",
      "+--------------------------+\n",
      "|              -16.67741935|\n",
      "+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Outra tarefa comum é observar a diferença entre duas datas. Podemos fazer isso com a função datediff \n",
    "# que retornará o número de dias entre duas datas. Na maioria das vezes apenas nos preocupamos com os dias, \n",
    "# e como o número de dias varia de mês para mês, também existe uma função, months_between, \n",
    "# que fornece o número de meses entre duas datas:\n",
    "\n",
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    "    .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n",
    "\n",
    "dateDF.select(to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "              to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    "            .select(months_between(col(\"start\"), col(\"end\"))).show(1)\n",
    "\n",
    "# SELECT to_date('2016-01-01'), months_between('2016-01-01', '2017-01-01'),\n",
    "# datediff('2016-01-01', '2017-01-01')\n",
    "# FROM dateTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A função to_date permite converter uma string em uma data, opcionalmente com um formato especificado. \n",
    "# Nós especificamos nosso formato no Java SimpleDateFormat, que será importante para referenciar se você usar esta função:\n",
    "\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n",
    "    .select(to_date(col(\"date\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# O Spark não lançará um erro se não puder analisar a data; em vez disso, apenas retornará null. \n",
    "# Isso pode ser um pouco complicado em pipelines maiores porque você pode estar esperando seus \n",
    "# dados em um formato e obtê-los em outro.\n",
    "\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achamos que essa é uma situação especialmente complicada para bugs, pois algumas datas podem corresponder ao formato correto, enquanto outras não. No exemplo anterior, observe como a segunda data aparece como 11 de dezembro em vez do dia correto, 12 de novembro. O Spark não gera um erro porque não pode saber se os dias estão misturados ou se a linha específica está incorreta. Vamos corrigir esse pipeline, passo a passo, e criar uma maneira robusta de evitar totalmente esses problemas. O primeiro passo é lembrar que precisamos especificar nosso formato de data de acordo com o padrão __Java SimpleDateFormat__. Vamos usar duas funções para corrigir isso: ```to_date``` e ```to_timestamp```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "\n",
    "cleanDateDF = spark.range(1).select(\\\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\\\n",
    "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT to_date(date, 'yyyy-dd-MM'), to_date(date2, 'yyyy-dd-MM'), to_date(date)\n",
    "# FROM dateTable2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|to_timestamp(`date`, 'yyyy-dd-MM')|\n",
      "+----------------------------------+\n",
      "|               2017-11-12 00:00:00|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agora, vamos usar um exemplo de to_timestamp, que sempre exige que um formato seja especificado:\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT to_timestamp(date, 'yyyy-dd-MM'), to_timestamp(date2, 'yyyy-dd-MM')\n",
    "# FROM dateTable2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correspondência entre datas e timestamp é simples em todos as linguagens - no SQL, faríamos isso no seguinte maneira:\n",
    "# SELECT cast(to_date(\"2017-01-01\", \"yyyy-dd-MM\") as timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Depois de termos nossa data ou registro de data e hora no formato e tipo corretos, comparar entre eles \n",
    "# é realmente muito fácil. Precisamos apenas usar um tipo de data / timestamp ou especificar nossa string \n",
    "# de acordo com o formato correto de aaaa-MM-dd, se estivermos comparando uma data:\n",
    "\n",
    "cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Um ponto de menor importância é que também podemos definir isso como uma string, que o Spark analisa para um literal:\n",
    "cleanDateDF.filter(col(\"date2\") > \"'2017-12-12'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Material baseado em exemplos do livro __Spark - The Definitive Guide. Bill Chambers e Matei Zaharia__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
