{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 5 - Operações estruturadas - Parte 2\n",
    "<br>\n",
    "<div style=\"text-align: justify\">Por definição, um DataFrame consiste em uma série de registros (como linhas em uma tabela), que são do tipo Linha e um número de colunas (como colunas em uma planilha) que representam uma expressão de cálculo que pode ser executada em cada registro individual do conjunto de dados. Os esquemas definem o nome e o tipo de dados em cada coluna. O particionamento do DataFrame define o layout da distribuição física do DataFrame ou do Conjunto de Dados no cluster. O esquema de particionamento define como isso é alocado. Você pode definir isso para ser baseado em valores em uma determinada coluna ou não-deterministicamente.</div>\n",
    "\n",
    "### Sumário\n",
    " 1. __Maiúsculas e minúsculas__ \n",
    " 2. __drop()__ - Removendo colunas\n",
    " 3. __cast()__ - Mudando o tipo da coluna\n",
    " 4. __filter() / where()__ - Filtrando linhas\n",
    " 5. __distinc()__ - Acessando valores não duplicados\n",
    " 6. __sample()__ - Amostras aleatórias\n",
    " 7. __randomSplit()__ - Divisões aleatórias\n",
    " 8. __union()__ - Concatenando e adicionando Linhas\n",
    " 9. __sort() / orderBy()__ - Ordenando Linhas\n",
    " 10. __limit()__ - Limitando extração\n",
    " 11. __repartition() / coalesce()__ - Repartição e coalesce\n",
    " 12. __collect() / take() / show() / toLocalIterator()__ - Coletando dados para o Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo um Dataframe de trabalho e importando bilbiotecas necessárias\n",
    "path = \"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/json/2015-summary.json\"\n",
    "df = spark.read.format(\"json\").load(path)\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maiúsculas e minúsculas\n",
    "\n",
    "Por padrão, o Spark não é sensível a maiúsculas e minúsculas. Para torná-lo sensível, deve-se mudar o seguinte parâmetro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.sql.caseSensitive=true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop() - Removendo colunas\n",
    "\n",
    "Por padrão, o Spark não é sensível a maiúsculas e minúsculas. Para torná-lo sensível, deve-se mudar o seguinte parâmetro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminando múltiplas colunas \n",
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cast() - Mudando o tipo da coluna\n",
    "\n",
    "Às vezes, precisamos converter de um tipo para outro. Por exemplo, se tivermos um conjunto de StringType que deve ser convertido para inteiro. Podemos converter colunas de um tipo para outro, lançando a coluna de um tipo para outro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"count2\", F.col(\"count\").cast(\"string\"))\n",
    "# Equivalente em SQL:\n",
    "# SELECT *, cast(count as long) AS count2 FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter( ) / where( ) - Filtrando linhas\n",
    "\n",
    "Para filtrar linhas, criamos uma expressão que é avaliada como verdadeira ou falsa. Em seguida, filtramos as linhas com uma expressão igual a _false_. A maneira mais comum de fazer isso com DataFrames é criar uma expressão como uma String ou criar uma expressão usando um conjunto de manipulações de coluna. Existem dois métodos para realizar esta operação: você pode usar __where()__ ou __filter()__ e ambos executarão a mesma operação e aceitarão os mesmos tipos de argumentos quando usados com DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"count\") < 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"count < 2\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente em SQL:\n",
    "# SELECT * FROM dfTable WHERE count < 2 LIMIT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instintivamente, você pode querer colocar vários filtros na mesma expressão. Embora isso seja possível, nem sempre é útil, porque o Spark executa automaticamente todas as operações de filtragem ao mesmo tempo, independentemente da ordem do filtro. Isso significa que, se você quiser especificar vários filtros AND, encadeie-os sequencialmente e deixe o Spark manipular o restante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente em SQL:\n",
    "# SELECT * FROM dfTable WHERE count < 2 AND ORIGIN_COUNTRY_NAME != \"Croatia\" LIMIT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct( ) - Acessando valores não duplicados\n",
    "\n",
    "Um caso de uso muito comum é extrair os valores exclusivos ou distintos em um DataFrame. Esses valores podem estar em uma ou mais colunas. A maneira como fazemos isso é usando o método __distinct()__, que nos permite deduplicar as linhas que estão nesse DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()\n",
    "# Equivalente em SQL:\n",
    "# SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) FROM dfTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()\n",
    "# Equivalente em SQL:\n",
    "# SELECT COUNT(DISTINCT ORIGIN_COUNTRY_NAME) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample() - Amostras aleatórias\n",
    "\n",
    "Às vezes, você pode querer apenas colher amostras aleatórias de registros do seu DataFrame. Você pode fazer isso usando o método __sample()__ em um DataFrame, o que possibilita que você especifique uma fração de linhas para extrair de um DataFrame e, se deseja amostrar com ou sem substituição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomSplit() - Divisões aleatórias\n",
    "\n",
    "Pode ser útil dividir seu DataFrame em \"divisões\" aleatórias do DataFrame original. Isso é frequentemente usado com algoritmos de aprendizado de máquina para criar conjuntos de treinamento, validação e teste. Neste próximo exemplo, dividiremos nosso DataFrame em dois DataFrames diferentes, definindo os pesos pelos quais dividiremos o DataFrame (esses são os argumentos para a função). Como esse método foi desenvolvido para ser randomizado, também especificaremos uma semente(seed) (basta substituir a semente por um número de sua escolha no bloco de código)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames = df.randomSplit([0.25, 0.75], seed) # Dividindo as amostras em 25% e 75% do dataframe\n",
    "dataFrames[0].count() > dataFrames[1].count()   # Verificando se a 1ª amostra é maior que a 2ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union() - Concatenando e adicionando Linhas\n",
    "\n",
    "DataFrames são imutáveis, isso significa que os usuários não podem adicionar elementos a um DataFrame porque isso seria alterá-lo. Para adicionar elementos a um DataFrame, você deve unir o DataFrame original com o novo DataFrame. Isso apenas concatena os dois DataFrames. Para unir dois DataFrames, você deve ter certeza de que eles têm o mesmo esquema e número de colunas; caso contrário, a união falhará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "|    United States|          Lithuania|    1|\n",
      "|    United States|           Bulgaria|    1|\n",
      "|    United States|            Georgia|    1|\n",
      "|    United States|            Bahrain|    1|\n",
      "|    United States|   Papua New Guinea|    1|\n",
      "|    United States|         Montenegro|    1|\n",
      "|    United States|            Namibia|    1|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Salvando o schema do DataFrame original em uma variável\n",
    "schema = df.schema\n",
    "\n",
    "# Criando novas linhas\n",
    "newRows = [Row(\"New Country\", \"Other Country\", 5L), Row(\"New Country 2\", \"Other Country 3\", 1L)]\n",
    "\n",
    "# Criando os RDDs com os dados da variável\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "\n",
    "# Criando o DataFrame e aplicando o schema do DataFrame original\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "\n",
    "# Unindo os DataFrames com as regras de negócio dentro das clausulas where\n",
    "df.union(newDF).where(\"count = 1\").where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort() / orderBy() - Ordenando Linhas\n",
    "\n",
    "Quando classificamos os valores em um DataFrame, sempre queremos classificar os valores maiores ou menores no topo de um DataFrame. Existem duas operações equivalentes para fazer esse tipo e ordenar por que funcionam exatamente da mesma maneira. Eles aceitam expressões e strings de colunas, além de várias colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordenando pela coluna \"count\" e exibindo todo o conteúdo do dataframe\n",
    "df.sort(\"count\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordenando pela coluna \"count\" e depois pela \"DEST_COUNTRY_NAME\" e exibindo todo o conteúdo do dataframe\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordenando com a mesma regra da celula anterior através da função col()\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para especificar explicitamente a direção de classificação, você precisa usar as funções __asc( )__ e __desc( )__ se estiver operando em uma coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(expr(\"count desc\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente em SQL:\n",
    "# SELECT * FROM dfTable ORDER BY count DESC, DEST_COUNTRY_NAME ASC LIMIT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma dica avançada é usar __asc_nulls_first__, __desc_nulls_first__, __asc_nulls_last__ ou __desc_nulls_last__ para especificar onde você gostaria que seus valores nulos aparecessem em um DataFrame ordenado. Para fins de otimização, às vezes é aconselhável classificar em cada partição antes de outro conjunto de transformações. Você pode usar o método __sortWithinPartitions()__ para fazer isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/json/*-summary.json\"\n",
    "spark.read.format(\"json\").load(path).sortWithinPartitions(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### limit() - Limitando extração\n",
    "\n",
    "Muitas vezes, você pode querer restringir o que você extrair de um DataFrame; por exemplo, você pode querer apenas os dez primeiros registros de algum DataFrame. Você pode fazer isso usando o método __limit( )__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente em SQL:\n",
    "# SELECT * FROM dfTable ORDER BY count desc LIMIT 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repartition() / coalesce() - Repartição e coalesce\n",
    "\n",
    "Outra importante oportunidade de otimização é particionar os dados de acordo com algumas colunas filtradas com freqüência, que controlam o layout físico dos dados no cluster, incluindo o esquema de particionamento e o número de partições. A repartição incorrerá em uma mistura completa dos dados, independentemente de ser necessário. Isso significa que você normalmente só deve reparticionar quando o número futuro de partições for maior que o número atual de partições ou quando você estiver procurando particionar por um conjunto de colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se você sabe que vai filtrar uma determinada coluna com frequência, pode valer a pena reparticionar \n",
    "# com base nessa coluna.\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Especificando manualmente o número de partições \n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coalesce, por outro lado, não incorrerá em um shuffle completo e tentará combinar partições. \n",
    "# Esta operação embaralha os seus dados em cinco partições com base no nome do país de destino e, \n",
    "# em seguida, une-as (sem um shuffle completo).\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect() / take() / show() / toLocalIterator() - Coletando dados para o Driver\n",
    "\n",
    "O Spark mantém o estado do cluster no driver. Há momentos em que você deseja coletar alguns dos seus dados no driver para manipulá-los em sua máquina local. Até agora, não definimos explicitamente essa operação. No entanto, usamos vários métodos diferentes para fazer isso que são efetivamente todos iguais. __collect( )__ obtém todos os dados de todo o DataFrame, seleciona as primeiras __N__ linhas e mostra as várias linhas impressas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF = df.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME=u'Egypt', ORIGIN_COUNTRY_NAME=u'United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'India', count=62)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take funciona com a passagem de um inteiro como parãmetro de contagem\n",
    "collectDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exibe o Dataframe na tela\n",
    "collectDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# truncate = False, exibe na íntegra o conteúdo do DataFrame\n",
    "collectDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME=u'Egypt', ORIGIN_COUNTRY_NAME=u'United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'India', count=62),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME=u'Costa Rica', ORIGIN_COUNTRY_NAME=u'United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME=u'Senegal', ORIGIN_COUNTRY_NAME=u'United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME=u'Moldova', ORIGIN_COUNTRY_NAME=u'United States', count=1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coleta os valores presentes nos nós do cluster para o programa driver\n",
    "# *** Este comando deve ser usado com muito cuidado, pois se o tamanho do Dataframe que for coletado ***\n",
    "# *** para o driver for maior que a memória disponível, o problema driver ira \"crashar\". ***\n",
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há uma maneira adicional de coletar linhas para o driver para iterar todo o conjunto de dados. O método __toLocalIterator()__ coleta partições para o driver como um iterador. Esse método permite iterar sobre todo o conjunto de dados, partição por partição, de maneira serial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.chain at 0x7fbdcf155dd0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.toLocalIterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! Qualquer coleta de dados para o driver pode ser uma operação muito cara! Se você tiver um conjunto de dados grande e usar o collect( ), poderá interromper o driver. Se você usar toLocalIterator( ) e tiver partições muito grandes, poderá facilmente travar o nó do driver e perder o estado do seu aplicativo. Isso também é caro porque podemos operar em uma base um a um, em vez de executar o cálculo em paralelo !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Material baseado em exemplos do livro __Spark - The Definitive Guide. Bill Chambers e Matei Zaharia__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
