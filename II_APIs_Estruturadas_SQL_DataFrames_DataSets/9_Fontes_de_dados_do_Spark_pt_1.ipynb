{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 9 - Fontes de dados - Parte 1 \n",
    "<br>\n",
    "O Spark contém seis tipo de dados \"core\" mas pode trabalhar com diversos outros formatos externos escritos pela comunidade.\n",
    "\n",
    "- CSV\n",
    "- JSON\n",
    "- Parquet\n",
    "- ORC\n",
    "- JDBC/ODBC\n",
    "- Arquivos texto\n",
    "\n",
    "### Estrutura básica de leitura de arquivos\n",
    "A estrutura de leitura de dados básica do Spark é o DataFrameReader, usamos para acessá-la, o atributo:<br>\n",
    "spark.read\n",
    "\n",
    "Após obtermos o DataFrame reader, especificamos diversos valores:\n",
    "- O formato\n",
    "- O _Schema_\n",
    "- O modo de leitura\n",
    "- E uma série de opções\n",
    "\n",
    "Cada tipo de dado possui uma especificação de carregamento diferente. devemos passar ao menos o caminho onde obter esse arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'someSchema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c5dda1886e59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Exemplo de carregamento de arquivo .csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"FAILFAST\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"path/to/files\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msomeSchema\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'someSchema' is not defined"
     ]
    }
   ],
   "source": [
    "# Exemplo de carregamento de arquivo .csv\n",
    "spark.read.format('csv')\\\n",
    "    .option(\"mode\",\"FAILFAST\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .option(\"path\",\"path/to/files\")\\\n",
    "    .option(someSchema)\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark's read modes\n",
    "Especifica o que acontece quando o Spark encontra registros com má formação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribui \"null\" a todos os campos corrompidos e coloca todos os\n",
    "# campos corrompidos em uma coluna de strings chamada _corrupt_record\n",
    ".option(\"mode\",\"permissive\")\n",
    "\n",
    "# Exclui a linha que contém registros com má formação.\n",
    ".option(\"mode\",\"dropMalformed\")\n",
    "\n",
    "# Falha imediatamente ao encontrar registros malformados.\n",
    ".option(\"mode\",\"failFast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrutura básica de gravação de arquivos\n",
    "A estrutura de gravação de dados básica do Spark é similar a leitura. No lugar do DataFrameReader usa-se o DataFrameWriter através do atribuito:\n",
    "dataFrame.writer\n",
    "\n",
    "Após obtermos o DataFrame writer, especificamos diversos valores:\n",
    "\n",
    "- O formato\n",
    "- E uma série de opções\n",
    "- O Particionamento\n",
    "- Bucketing\n",
    "- Ordenação\n",
    "- O modo de escrita\n",
    "\n",
    "Cada tipo de dado possui uma especificação de carregamento diferente. devemos passar ao menos o caminho onde obter esse arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-98bf018b748f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Exemplo de salvamento de arquivo .csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dateformat\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"yyyy-MM-dd\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"path/to/files\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "# Exemplo de salvamento de arquivo .csv\n",
    "dataframe.write.format('csv')\\\n",
    "    .option(\"mode\",\"overwrite\")\\\n",
    "    .option(\"dateformat\",\"yyyy-MM-dd\")\\\n",
    "    .option(\"path\",\"path/to/files\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark's save modes\n",
    "Lista formas de salvamento do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciona os arquivos de saída em uma lista de arquivos ja existentes na dada localização.\n",
    ".option(\"mode\",\"append\")\n",
    "\n",
    "# Sobrescreve completamente qualquer dado existente no arquivo descrimindo\n",
    ".option(\"mode\",\"overwrite\")\n",
    "\n",
    "# Levanta um erro e gera falha na escrita do arquivo se os dados ou o arquivo já existe na dada localização. \n",
    ".option(\"mode\",\"errorIfExists\")\n",
    "\n",
    "# Se os dados ou arquivo já existirem, nada é feito com o DataFrame atual.\n",
    ".option(\"mode\",\"ignore\")\n",
    "\n",
    ".options(\"sep\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquivos CSV\n",
    "#### Opções a serem incluídas no     .options( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>Escrita/Leitura|<center>Chave|<center>Valores Potenciais|<center>Padrão|<center>Descrição|\n",
    "|:---------------------:|:------------:|:------------------------:|:------------:|:---------------:|\n",
    "|<center>Ambos   |<center>sep                        |<center>Qualquer string  |<center> ,|<center>Um caracter simples que é usado como separador para cada campo e valor|\n",
    "|<center>Ambos   |<center>header                     |<center>true, false      | <center>false|<center> Uma flag booleana que declara se a primeira linha do arquivo contém os nosmes das colunas|\n",
    "|<center>Leitura |<center>escape                     |<center>Qualquer string  | <center>\\ |<center>O caractere que o Spark deve usar para evitar outros caracteres no arquivo.|\n",
    "|<center> Leitura|<center>inferSchema                |<center>true, false      | <center>false|<center>Especifica se o Spark deve inferir tipos de colunas ao ler o arquivo.|\n",
    "|<center> Leitura|<center>ignoreLeadingWhiteSpace    |<center>true, false      | <center>false|<center>Declara se os espaços iniciais dos valores que estão sendo lidos devem ser ignorados.|\n",
    "|<center> Leitura|<center> ignoreTrailingWhiteSpace  |<center>true, false      | <center>false|<center>Declara se os espaços finais dos valores que estão sendo lidos devem ser ignorados.|\n",
    "|<center> Ambos  |<center>nullValue                  |<center>Qualquer string  | <center> \" \"|<center>Declara qual caractere representa um valor nulo no arquivo.|\n",
    "|<center> Ambos  |<center>nanValue                   |<center>Qualquer string  | <center>NaN|<center>Declara qual caractere representa um NaN ou um caractere ausente no arquivo CSV.|\n",
    "|<center> Ambos  |<center>positiveInf                |<center>Qualquer string  | <center>Inf|<center>Declara que caractere (s) representam um valor infinito positivo.|\n",
    "|<center> Ambos  |<center>negativeInf                |<center>Qualquer string  | <center>-Inf|<center>Declara que caractere (s) representam um valor infinito negativo.|\n",
    "|<center> Ambos  |<center>compression or codec       |<center>None,uncompressed,bzip2, deflate,gzip, lz4 ou snapp| <center>none|<center>Declara qual codec de compactação o Spark deve usar para ler ou gravar o arquivo.|\n",
    "|<center> Ambos  |<center>dateFormat                 |<center>Qualquer string(SimpleDataFormat) |<center>yyyy-MM-dd|<center>Declara o formato de data para qualquer coluna que seja do tipo de data.|\n",
    "|<center> Ambos  |<center>timestampFormat            |<center>Qualquer string(SimpleDataFormat) |<center>yyyy-MM-dd’T’HH:mm:ss.SSSZZ |<center>Declara o formato do registro de data e hora para qualquer coluna que seja do tipo timestamp.|\n",
    "|<center> Leitura|<center>maxColumns                 |<center>Qualquer inteiro | <center>20480 |<center>Declara o número máximo de colunas no arquivo.|\n",
    "|<center> Leitura|<center>maxCharsPerColumn          |<center>Qualquer inteiro | <center>1000000 |<center>Declara o número máximo de caracteres em uma coluna.|\n",
    "|<center> Leitura|<center>escapeQuotes               |<center>true, false      | <center>true |<center>Declara se o Spark deve escapar de cotações encontradas em linhas.|\n",
    "|<center> Leitura|<center>maxMalformedLogPerPartition|<center>Qualquer inteiro | <center>10 |<center>Define o número máximo de linhas malformadas que o Spark registrará em cada partição. Registros malformados além desse número serão ignorados.|\n",
    "|<center> Escrita|<center>quoteAll                   |<center>true, false      | <center>false |<center>Especifica se todos os valores devem ser colocados entre aspas, ao contrário de apenas valores de escape que possuem um caractere de aspas.|\n",
    "|<center> Leitura|<center>multiLine                  |<center>true, false      | <center>false |<center>Essa opção permite que você leia arquivos CSV de várias linhas, em que cada linha lógica no arquivo CSV pode abranger várias linhas no próprio arquivo.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo arquivos CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Podemos utiliar as opções:\n",
    ".format(\"csv\")\n",
    ".read.csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "\n",
    "# Criando os Schenas que serão aplicados ao arquivo \".csv\" de leitura\n",
    "\n",
    "myManualSchema = StructType([\\\n",
    "                             StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\\\n",
    "                             StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\\\n",
    "                             StructField(\"COUNT\", IntegerType(), False)\\\n",
    "                            ])\n",
    "\n",
    "# Definindo o tipo arquivo como \"csv\", a primeira linha como cabeçalho, falhando na execução caso \n",
    "# o arquivo contenha registros malformados, aplicando schema definido acima, lendo o arquivo \".csv\" na pasta\n",
    "# passada como parâmetro e exibindo os 5 primeiros registros. A variável df conterá o dataFrame correspondente ao arquivo\n",
    "# aberto.\n",
    "df_csv = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"mode\", \"failFast\")\\\n",
    "    .schema(myManualSchema)\\\n",
    "    .load(\"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/csv/2010-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|COUNT|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "|    United States|          Singapore|   25|\n",
      "|    United States|            Grenada|   54|\n",
      "|       Costa Rica|      United States|  477|\n",
      "|          Senegal|      United States|   29|\n",
      "|    United States|   Marshall Islands|   44|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gravando arquivos CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrevendo arquivos \".csv\" \n",
    "df_csv.write.format(\"csv\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"sep\", \"\\t\")\\\n",
    "    .save(\"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/csv/my-tsv-file.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquivos JSON\n",
    "#### Opções a serem incluídas no     .options( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>Escrita/Leitura|<center>Chave|<center>Valores Potenciais|<center>Padrão|<center>Descrição|\n",
    "|:---------------------:|:------------:|:------------------------:|:------------:|:---------------:|\n",
    "|<center>Ambos   |<center>compression or codec               |<center>None, uncompressed, bzip2, deflate, gzip, lz4, ou snapppy|<center>none|<center>Compressão ou codec para usar ao salvar em arquivo. Esse pode ser um dos nomes abreviados que não diferenciam maiúsculas de minúsculas (none, bzip2, gzip, lz4, snappy e deflate).|\n",
    "|<center>Ambos   |<center>dateFormat                         |<center>Qualquer string(SimpleDataFormat)| <center>yyyy-MM-dd|<center>Define a string que indica um formato de data. Os formatos de data personalizados seguem os formatos em java.text.SimpleDateFormat. Isso se aplica ao tipo de data. Se None estiver definido, ele usa o valor padrão, aaaa-MM-dd.|\n",
    "|<center>Ambos   |<center>timestampFormat                    |<center>Qualquer string(SimpleDataFormat)  | <center>yyyy-MM-dd’T’HH: mm:ss.SSSZZ|<center>Define a string que indica um formato de registro de data e hora. Os formatos de data personalizados seguem os formatos em java.text.SimpleDateFormat. Isso se aplica ao tipo de registro de data e hora. Se None estiver definido, ele usa o valor padrão, aaaa-MM-dd'T'HH: mm: ss.SSSXXX.|\n",
    "|<center> Leitura|<center>primitiveAsString                  |<center>true, false      | <center>false|<center>Define a string que indica um formato de registro de data e hora. Os formatos de data personalizados seguem os formatos em java.text.SimpleDateFormat. Isso se aplica ao tipo de registro de data e hora. Se None estiver definido, ele usa o valor padrão, aaaa-MM-dd'T'HH: mm: ss.SSSXXX.|\n",
    "|<center> Leitura|<center>allowComments                      |<center>true, false      | <center>false|<center>Ignora o comentário do estilo Java / C ++ em registros JSON. Se None estiver definido, ele usa o valor padrão, false.|\n",
    "|<center> Leitura|<center> allowUnquotedFieldNames           |<center>true, false      | <center>false|<center>Permite nomes de campo JSON sem nome. Se None estiver definido, ele usa o valor padrão, false.|\n",
    "|<center> Leitura|<center>allowSingleQuotes                  |<center>true, false  | <center> true|<center>Permite cotações simples além de aspas duplas. Se None estiver definido, ele usa o valor padrão, true.|\n",
    "|<center> Leitura|<center>allowNumericLeadingZeros           |<center>true, false  | <center>false|<center>permite levar zeros em números (por exemplo, 00012). Se None estiver definido, ele usa o valor padrão, false.|\n",
    "|<center> Leitura|<center>allowBackslashEscapingAnyCharacter |<center>true, false  | <center>false|<center>permite aceitar a citação de todos os caracteres usando o mecanismo de citação de barra invertida. Se None estiver definido, ele usa o valor padrão, false.|\n",
    "|<center> Leitura|<center>columnNameOfCorruptRecord          |<center>Qualquer string  | <center>Valor de spark.sql.column&NameOfCorruptRecord|<center>permite renomear o novo campo com uma string malformada criada pelo modo PERMISSIVO. Isso substitui spark.sql.columnNameOfCorruptRecord. Se None estiver definido, ele usa o valor especificado em spark.sql.columnNameOfCorruptRecord.|\n",
    "|<center> Leitura|<center>multiLine                          |<center>true, false| <center>false|<center>analisar um registro, que pode abranger várias linhas, por arquivo. Se None estiver definido, ele usa o valor padrão, false.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo arquivos JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Podemos utiliar as opções:\n",
    ".format(\"json\")\n",
    ".read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.format(\"json\").option(\"mode\",\"failFast\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/json/2010-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "|    United States|          Singapore|   25|\n",
      "|    United States|            Grenada|   54|\n",
      "|       Costa Rica|      United States|  477|\n",
      "|          Senegal|      United States|   29|\n",
      "|    United States|   Marshall Islands|   44|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gravando arquivos JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrevendo arquivos \".json\" \n",
    "df_json.write.format(\"json\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(\"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/csv/my-json-file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquivos Parquet\n",
    "\n",
    "O Formato Parquet tem poucas opções porque força seu próprio _schema_ na gravação dos arquivos. Uma precaução deve ser observada na gravação de arquivos parquet, quanto a versão do Spark a ser utilizada, especialmente as mais antigas. O format parquet é otimizado para ultilização com Spark.\n",
    "\n",
    "#### Opções a serem incluídas no     .options( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>Escrita/Leitura|<center>Chave|<center>Valores Potenciais|<center>Padrão|<center>Descrição|\n",
    "|:---------------------:|:------------:|:------------------------:|:------------:|:---------------:|\n",
    "|<center>Escrita   |<center>compression or codec               |<center>None, uncompressed, bzip2, deflate, gzip, lz4, ou snapppy|<center>none|<center>Compressão ou codec para usar ao salvar e ao ler o arquivo. Esse pode ser um dos nomes abreviados que não diferenciam maiúsculas de minúsculas.|\n",
    "|<center>Leitura   |<center>mergeSchema                         |<center>true, false| <center>Valor da configuração: spark.sql.parquet.mergeSchema|<center>Podemos adicionar colunas de forma incremental em arquivos parquet ja escritos na mesma tabela/arquivo. Use essa opção para habilitar e disabilitar essa funcionalidade|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo arquivos Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Podemos utiliar as opções:\n",
    ".format(\"parquet\")\n",
    ".read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.format(\"parquet\")\\\n",
    "    .load(\"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/parquet/2010-summary.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gravando arquivos Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.write.format(\"parquet\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(\"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/parquet/my-parquet-files.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquivos ORC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ORC é um formato de arquivo columnar autodescritivo e com reconhecimento de tipos, projetado para cargas de trabalho do Hadoop. Ele é otimizado para leituras de streaming grandes, mas com suporte integrado para localizar rapidamente as linhas necessárias. O ORC na verdade não tem opções para ler dados porque o Spark entende muito bem o formato do arquivo. Uma pergunta frequente é: Qual é a diferença entre o ORC e o Parquet? Na maior parte, eles são bem parecidos; A diferença fundamental é que o Parquet é otimizado para uso com o Spark, enquanto o ORC é otimizado para o Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo arquivos ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orc = spark.read.format(\"orc\")\\\n",
    "    .load(\"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/orc/2010-summary.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gravando arquivos ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.write.format(\"orc\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(\"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/parquet/my-json-file.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquivos texto\n",
    "\n",
    "Spark também permite ler em arquivos de texto simples. Cada linha no arquivo se torna um registro no DataFrame. Então cabe a você transformá-lo de acordo. Como exemplo de como você faria isso, suponha que você precise analisar alguns arquivos de log do Apache para um formato mais estruturado ou talvez queira analisar alguns textos simples para o processamento de linguagem natural. Os arquivos de texto são um ótimo argumento para a API do Conjunto de Dados devido à sua capacidade de aproveitar a flexibilidade dos tipos nativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo arquivos texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = spark.read.text(\"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/csv/2010-summary.csv\")\\\n",
    "    .selectExpr(\"split(value, ',') as rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                rows|\n",
      "+--------------------+\n",
      "|[DEST_COUNTRY_NAM...|\n",
      "|[United States, R...|\n",
      "|[United States, I...|\n",
      "|[United States, I...|\n",
      "|[Egypt, United St...|\n",
      "|[Equatorial Guine...|\n",
      "|[United States, S...|\n",
      "|[United States, G...|\n",
      "|[Costa Rica, Unit...|\n",
      "|[Senegal, United ...|\n",
      "|[United States, M...|\n",
      "|[Guyana, United S...|\n",
      "|[United States, S...|\n",
      "|[Malta, United St...|\n",
      "|[Bolivia, United ...|\n",
      "|[Anguilla, United...|\n",
      "|[Turks and Caicos...|\n",
      "|[United States, A...|\n",
      "|[Saint Vincent an...|\n",
      "|[Italy, United St...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gravando arquivos Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|United States,Rom...|\n",
      "|United States,Ire...|\n",
      "|United States,Ind...|\n",
      "|Egypt,United Stat...|\n",
      "|Equatorial Guinea...|\n",
      "|United States,Sin...|\n",
      "|United States,Gre...|\n",
      "|Costa Rica,United...|\n",
      "|Senegal,United St...|\n",
      "|United States,Mar...|\n",
      "|Guyana,United Sta...|\n",
      "|United States,Sin...|\n",
      "|Malta,United Stat...|\n",
      "|Bolivia,United St...|\n",
      "|Anguilla,United S...|\n",
      "|Turks and Caicos ...|\n",
      "|United States,Afg...|\n",
      "|Saint Vincent and...|\n",
      "|Italy,United Stat...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text = spark.read.text(\"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/csv/2010-summary.csv\")\n",
    "df_text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.select(\"value\")\\\n",
    "    .write.text(\"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/csv//simple-text-file.txt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferenciando arquivos do sistemas de arquivos local e sistema de arquivos distribuido (HDFS)\n",
    "Por padrão, o Spark acessa dados do HDFS na pasta definida pelo arquivo hive-sites.conf \"/apps/hive/warehouse/\". Podem ocorrer mudanças segundo a distribuição e as versões do Hadoop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando arquivos localmente:\n",
    "path = \"file:///raiz/caminho/para/a/pasta/arquivo.ext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando arquivos no hdfs:\n",
    "path = \"/raiz/caminho/para/o/HDFS/arquivo.ext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passagem do caminho de leitura\n",
    "\n",
    "# Modo 1\n",
    ".load(path)\n",
    "\n",
    "# Modo 2\n",
    ".option(\"path\", path)\\\n",
    ".load() # ou .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Material baseado em exemplos do livro __Spark - The Definitive Guide. Bill Chambers e Matei Zaharia__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
