{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 5 - Operações estruturadas - Parte 1\n",
    "<br>\n",
    "<div style=\"text-align: justify\">Por definição, um DataFrame consiste em uma série de registros (como linhas em uma tabela), que são do tipo Linha e um número de colunas (como colunas em uma planilha) que representam uma expressão de cálculo que pode ser executada em cada registro individual do conjunto de dados. Os esquemas definem o nome e o tipo de dados em cada coluna. O particionamento do DataFrame define o layout da distribuição física do DataFrame ou do Conjunto de Dados no cluster. O esquema de particionamento define como isso é alocado. Você pode definir isso para ser baseado em valores em uma determinada coluna ou não-deterministicamente.</div>\n",
    "\n",
    "### Sumário\n",
    " 1. __.schema__ \n",
    " 2. __Colunas e expressões__\n",
    " 3. __col() / column()__ - Colunas\n",
    " 4. __expr()__ Expressões \n",
    " 5. __Colunas como expressões__\n",
    " 6. __first()__ Registros e linhas \n",
    " 7. __Row()__ - Criando Linhas \n",
    " 8. __createDataFrame()__ - Criando DataFrames \n",
    " 9. __select() / selectExpr()__ -  \n",
    " 10. __lit()__ - Convertendo para tipos Spark (Literais) \n",
    " 11. __withColumn()__ - Adicionando colunas\n",
    " 12. __withColumnRenamed()__ - Renomeando colunas\n",
    " 13. __Caracteres reservados e palavras chaves__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo um Dataframe de trabalho\n",
    "path = \"file:///root/sparkcurso/Spark_Definitive_Guide/data/flight-data/json/2015-summary.json\"\n",
    "df = spark.read.format(\"json\").load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrando as 5 primeiras linhas do dataframe\n",
    "df.show(5)\n",
    "# Mostrando as 5 primeiras linhas do dataframe mostrando o conteúdo completo da célula\n",
    "df.show(5, truncate=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema\n",
    "<br>\n",
    "<div style=\"text-align: justify\">Um schema define os nomes e tipos de coluna de um DataFrame. Podemos deixar uma fonte de dados definir o schema (chamado schema-on-read) ou podemos defini-lo explicitamente por nós mesmos.\n",
    "\n",
    "Um esquema é um _StructType_ composto de vários campos, _StructFields_, que possuem um nome, tipo, um sinalizador Booleano que especifica se essa coluna pode conter valores ausentes ou nulos e, finalmente, os usuários podem especificar opcionalmente metadados associados a essa coluna. Os metadados são uma maneira de armazenar informações sobre essa coluna (o Spark usa isso em sua biblioteca de aprendizado de máquina).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrando o Schema do DataFrame\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descrevendo a estrutura do DataFrame\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colunas e expressões\n",
    "<br>\n",
    "<div style=\"text-align: justify\">As colunas no Spark são semelhantes às colunas de uma planilha, do dataframe R ou do DataFrame do pandas. Você pode selecionar, manipular e remover colunas de DataFrames e essas operações são representadas como expressões. Para Spark, as colunas são construções lógicas que simplesmente representam um valor calculado por registro por meio de uma expressão. Isso significa que para ter um valor real para uma coluna, precisamos ter uma linha; e para ter uma linha, precisamos ter um DataFrame. Você não pode manipular uma coluna individual fora do contexto de um DataFrame; você deve usar as transformações do Spark em um DataFrame para modificar o conteúdo de uma coluna.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<someColumnName>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Há várias maneiras diferentes de construir e referenciar colunas, mas as duas maneiras mais simples \n",
    "# são usar as funções col ou column. \n",
    "# Para usar uma dessas funções, você passa um nome de coluna:\n",
    "from pyspark.sql.functions import col, column\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências explícitas a colunas\n",
    "<br>\n",
    "<div style=\"text-align: justify\">Se você precisar se referir a uma coluna específica do DataFrame, poderá usar o método col no DataFrame específico. Isso pode ser útil quando você está executando uma junção e precisa se referir a uma coluna específica em um DataFrame que pode compartilhar um nome com outra coluna no DataFrame associado. Como um benefício adicional, o Spark não precisa resolver essa coluna porque fizemos isso para o Spark:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressões\n",
    "<br>\n",
    "<div style=\"text-align: justify\">Nós mencionamos anteriormente que colunas são expressões, mas o que é uma expressão? Uma expressão é um conjunto de transformações em um ou mais valores em um registro em um DataFrame. Pense nisso como uma função que usa como entrada um ou mais nomes de coluna, resolve-os e aplica-se potencialmente mais expressões para criar um único valor para cada registro no conjunto de dados. É importante ressaltar que esse “valor único” pode na verdade ser um tipo complexo, como um Map ou Array.<div>\n",
    "\n",
    "#### Colunas como expressões\n",
    "<br>\n",
    "<div style=\"text-align: justify\">Colunas fornecem um subconjunto da funcionalidade de expressão. Se você usar col () e desejar executar transformações nessa coluna, deverá executar aquelas na referência dessa coluna. Ao usar uma expressão, a função <b>expr</b> pode realmente analisar transformações e referências de coluna de uma seqüência de caracteres e, posteriormente, pode ser passada para outras transformações. Vamos ver alguns exemplos:</div>\n",
    "\n",
    "expr (\"someCol - 5\") é a mesma transformação que executar col (\"someCol\") - 5, ou mesmo<br>\n",
    "expr (\"someCol\") - 5. Isso ocorre porque o Spark compila esses itens em uma árvore lógica especificando a ordem das operações. Isso pode ser um pouco confuso no começo, mas lembre-se de alguns pontos importantes:\n",
    "- Colunas são apenas expressões.\n",
    "- Colunas e transformações dessas colunas são compiladas para o mesmo plano lógico que as expressões analisadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acessando as colunas de um dataFrame\n",
    "Às vezes, você precisa ver as colunas do DataFrame, o que pode ser feito usando algo como printSchema; no entanto, se você quiser acessar programaticamente colunas, poderá usar a propriedade columns para ver todas as colunas em um DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registros e linhas\n",
    "No Spark, cada linha em um DataFrame é um único registro. Spark representa esse registro como um objeto do tipo Row. Spark manipula objetos Row usando expressões de coluna para produzir valores utilizáveis. Objetos de linha representam internamente matrizes de bytes. A interface de matriz de bytes nunca é mostrada aos usuários porque usamos apenas expressões de coluna para manipulá-las."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Romania', count=15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lendo o primeiro registro(linha) do DataFrame\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando Linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode criar linhas instanciando manualmente um objeto _Row_ com os valores que pertencem a cada coluna. É importante notar que apenas os DataFrames têm esquemas. As próprias linhas não possuem esquemas. Isso significa que, se você criar uma Linha manualmente, deverá especificar os valores na mesma ordem que o esquema do DataFrame ao qual eles podem ser anexados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "None\n",
      "1\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Criando um registro\n",
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)\n",
    "\n",
    "print myRow[0]\n",
    "print myRow[1]\n",
    "print myRow[2]\n",
    "print myRow[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando DataFrames\n",
    "\n",
    "Também podemos criar DataFrames na hora, pegando um conjunto de linhas e convertendo-as em um DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "# Criando o DataFrame\n",
    "myManualSchema = StructType([\n",
    "StructField(\"some\", StringType(), True),\n",
    "StructField(\"col\", StringType(), True),\n",
    "StructField(\"names\", LongType(), False)\n",
    "])\n",
    "\n",
    "# Criando a linha(Registro)\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "\n",
    "# Inserindo o resgistro no DataFrame\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select / selectExpr\n",
    "\n",
    "__select__ e __selectExpr__ permitem que você faça consultas SQL em DataFrames equivalentes a uma tabela de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionando uma coluna no DataFrame e exibindo os dois primeiros campos\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionando duas colunas no DataFrame e exibindo os dois primeiros campos\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apresentando diversas formas de referenciar uma coluna\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(\n",
    "expr(\"DEST_COUNTRY_NAME\"),\n",
    "col(\"DEST_COUNTRY_NAME\"),\n",
    "column(\"DEST_COUNTRY_NAME\"))\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos até agora, a __expr__ é a referência mais flexível que podemos usar. Pode se referir a uma coluna simples ou a uma manipulação de string de uma coluna. Para ilustrar, vamos alterar o nome da coluna e, em seguida, alterá-lo novamente usando a palavra-chave __AS__ e, em seguida, o método de __alias__ na coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n",
    "# Equivalente em SQL: \n",
    "# SELECT DEST_COUNTRY_NAME as destination FROM dfTable LIMIT 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "# Retorna o nome da coluna alterado por expr() ao valor original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o __select__ seguido por uma série de __expr__ é um padrão tão comum, o Spark tem uma forma abreviada de fazer isso de maneira eficiente: __selectExpr__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|newColumnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "# Retorna o nome da coluna alterado por expr() ao valor original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos tratar o __selectExpr__ como uma maneira simples de criar expressões complexas que criam novos DataFrames. Na verdade, podemos adicionar qualquer instrução SQL não agregada válida e, contanto que as colunas sejam resolvidas, ela será válida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "\"*\", # todas as colunas originais\n",
    "\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Com a expressão select, também podemos especificar agregações em todo o DataFrame aproveitando as funções que temos.\n",
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo para tipos Spark (Literais)\n",
    "\n",
    "Às vezes, precisamos passar valores explícitos para o Spark, que são apenas um valor (em vez de uma nova coluna). Isso pode ser um valor constante ou algo que precisaremos comparar para mais tarde. A maneira como fazemos isso é através de literais. Isso é basicamente uma tradução do valor literal de uma dada linguagem de programação para uma que o Spark entenda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionando todas as colunas do DataFrame e adicionando uma coluna chamada \"One\" com o valor literal 1.\n",
    "from pyspark.sql.functions import lit\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT *, 1 as One FROM dfTable LIMIT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumn (Adicionando colunas)\n",
    "\n",
    "Há também uma maneira mais formal de adicionar uma nova coluna a um DataFrame, usando o método withColumn em nosso DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adicionando nova coluna \"nunberOne\" com o valor literal 1 em um DataFrame\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)\n",
    "\n",
    "# Equivalente em SQL:\n",
    "# SELECT *, 1 as numberOne FROM dfTable LIMIT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inserindo uma coluna com uma regra que seleciona elementos onde o país de origem é igual ao país de destino\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'Destination']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe que a função withColumn aceita dois argumentos: o nome da coluna e a expressão \n",
    "# que criará o valor para essa linha no DataFrame. \n",
    "# Curiosamente, também podemos renomear uma coluna dessa maneira.\n",
    "\n",
    "df.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\")).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumnRenamed (Renomeando colunas)\n",
    "\n",
    "Há também uma maneira mais formal de adicionar uma nova coluna a um DataFrame, usando o método withColumn em nosso DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renomeando a coluna \"DEST_COUNTRY_NAME\" para \"dest\"\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracteres reservados e palavras chaves\n",
    "\n",
    "Uma coisa que você pode encontrar é caracteres reservados, como espaços ou traços em nomes de colunas. Manipular estes significa escapar nomes de coluna apropriadamente. No Spark, fazemos isso usando caracteres (`). Vamos usar o withColumn, para criar uma coluna com caracteres reservados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não precisamos de caracteres de escape aqui porque o primeiro argumento para withColumn é apenas uma string \n",
    "# para o novo nome da coluna.\n",
    "dfWithLongColName = df.withColumn(\"This Long Column-Name\", expr(\"ORIGIN_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------+\n",
      "|This Long Column-Name|      new col|\n",
      "+---------------------+-------------+\n",
      "|              Romania|      Romania|\n",
      "|              Croatia|      Croatia|\n",
      "|              Ireland|      Ireland|\n",
      "|        United States|United States|\n",
      "|                India|        India|\n",
      "+---------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neste exemplo, no entanto, precisamos usar backticks porque estamos fazendo \n",
    "# referência a uma coluna em uma expressão.\n",
    "dfWithLongColName.selectExpr(\"`This Long Column-Name`\", \"`This Long Column-Name` as `new col`\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos nos referir a colunas com caracteres reservados (e não ignorá-los) se estivermos fazendo uma referência explícita de string para coluna, que é interpretada como literal em vez de expressão. Precisamos apenas \"escapar\" de expressões que usam caracteres reservados ou palavras-chave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This Long Column-Name']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Material baseado em exemplos do livro __Spark - The Definitive Guide. Bill Chambers e Matei Zaharia__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
